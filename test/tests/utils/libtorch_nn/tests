[Tests]
  design = 'LibtorchArtificialNeuralNet.md'
  issues = '#19571'

  [libtorch-nn-errors]
    requirement = 'The system shall throw an error'

    [wrong-activation]
      type = RunException
      input = 'test.i'
      cli_args = 'UserObjects/test/activation_functions=sad'
      expect_err = 'Invalid option \"sad\" in MultiMooseEnum.  Valid options \(not case-sensitive\) are \"relu sigmoid elu gelu linear\".'
      detail = 'when encountering an unsupported activation function for the neural network.'
      libtorch = true
      allow_test_objects = True
    []
    [wrong-activation-number]
      type = RunException
      input = 'test.i'
      cli_args = "UserObjects/test/activation_functions='relu sigmoid elu'"
      expect_err = 'The number of activation functions should be either one or the same as the number of hidden layers'
      detail = 'when encountering the wrong number of activation functions.'
      libtorch = true
      allow_test_objects = True
    []
  []
  [libtorch-nn-activation]
    requirement = 'The system shall be able to create and evaluate a neural network with'
    [relu]
      type = RunApp
      input = 'test.i'
      expect_out = 'My prediction\: -0\.1977'
      detail = 'relu activation functions.'
      libtorch = true
      allow_test_objects = True
    []
    [sigmoid]
      type = RunApp
      input = 'test.i'
      expect_out = 'My prediction\: -0\.1226'
      detail = 'sigmoid activation functions.'
      cli_args = "UserObjects/test/activation_functions='sigmoid sigmoid'"
      libtorch = true
      allow_test_objects = True
    []
    [gelu]
      type = RunApp
      input = 'test.i'
      expect_out = 'My prediction\: -0\.1649'
      detail = 'gelu activation functions.'
      cli_args = "UserObjects/test/activation_functions='gelu gelu'"
      libtorch = true
      allow_test_objects = True
    []
    [elu]
      type = RunApp
      input = 'test.i'
      expect_out = 'My prediction\: 0\.01 \*\n\-7\.7504'
      detail = 'elu activation functions.'
      cli_args = "UserObjects/test/activation_functions='elu elu'"
      libtorch = true
      allow_test_objects = True
    []
    [linear]
      type = RunApp
      input = 'test.i'
      expect_out = 'My prediction\: 0\.01 \*\n\-5\.4844'
      detail = 'linear activation functions.'
      cli_args = "UserObjects/test/activation_functions='linear linear'"
      libtorch = true
      allow_test_objects = True
    []
  []
[]
